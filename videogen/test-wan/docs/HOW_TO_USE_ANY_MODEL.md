# Using Different Video Generation Models

## üéØ Quick Answer: YES, the project can use ANY video model!

The architecture is **model-agnostic**. I've created two versions:

### Version 1: Single Model (Current)
- ‚úÖ Optimized for LTX-Video
- ‚úÖ Already working
- ‚úÖ Files: `python_backend/server.py`

### Version 2: Multi-Model (New!)
- ‚úÖ Supports 5+ models
- ‚úÖ Switch models via API
- ‚úÖ Files: `python_backend/server_multimodel.py`

---

## üì¶ Supported Models

### Currently Built-In (Multi-Model Version)

| Model | VRAM | Quality | Speed | Best For |
|-------|------|---------|-------|----------|
| **LTX-Video** | 12GB+ | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Medium | Best quality |
| **ModelScope** | 4GB+ | ‚≠ê‚≠ê‚≠ê‚≠ê | Fast | 4GB GPUs! |
| **ZeroScope v2** | 4GB+ | ‚≠ê‚≠ê‚≠ê | Fast | Low memory |
| **SVD** | 12GB+ | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Medium | Image‚ÜíVideo |
| **SVD-XT** | 12GB+ | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Medium | Long videos |

### Easy to Add

| Model | Compatibility | Notes |
|-------|--------------|-------|
| AnimateDiff | ‚úÖ Yes | Needs custom pipeline |
| VideoCrafter | ‚úÖ Yes | Similar to LTX-Video |
| Text2Video-Zero | ‚úÖ Yes | Zero-shot approach |
| CogVideo | ‚úÖ Yes | Chinese model |
| Your custom model | ‚úÖ Yes | Any Diffusers pipeline |

---

## üöÄ How to Use Multi-Model Version

### Option A: Start with Specific Model

```bash
# 1. Copy multi-model files
cp python_backend/server_multimodel.py python_backend/server.py
cp .env.multimodel .env

# 2. Choose your model in .env
VIDEO_MODEL=modelscope   # For 4GB GPU!
# or
VIDEO_MODEL=ltx-video    # For 12GB+ GPU
# or
VIDEO_MODEL=zeroscope    # For 4GB GPU
# or
VIDEO_MODEL=svd          # Image-to-video

# 3. Start normally
cd python_backend && source venv/bin/activate && python server.py
```

### Option B: Switch Models Dynamically (No Restart!)

```bash
# Start with any model
python server.py

# Then switch via API:
curl -X POST http://localhost:5000/api/switch-model \
  -H "Content-Type: application/json" \
  -d '{"model_name": "modelscope"}'

# Generate with new model
curl -X POST http://localhost:8080/api/v1/generate/text-to-video \
  -H "Content-Type: application/json" \
  -d '{"prompt": "A cat playing"}'
```

---

## üìä Model Comparison

### For 4GB GPU

```bash
# Use ModelScope - Best for 4GB!
VIDEO_MODEL=modelscope
ENABLE_GPU=true
MAX_FRAMES=32
DEFAULT_WIDTH=256
DEFAULT_HEIGHT=256
```

**Performance:**
- 256x256, 32 frames: ~60 seconds
- Quality: Good (not as good as LTX but acceptable)
- Actually works on 4GB!

### For 12GB+ GPU

```bash
# Use LTX-Video - Best quality
VIDEO_MODEL=ltx-video
ENABLE_GPU=true
MAX_FRAMES=64
DEFAULT_WIDTH=512
DEFAULT_HEIGHT=512
```

**Performance:**
- 512x512, 64 frames: ~90 seconds
- Quality: Excellent
- Best results

### For Image-to-Video

```bash
# Use Stable Video Diffusion
VIDEO_MODEL=svd
ENABLE_GPU=true
```

**Use case:**
- Take a still image
- Animate it into video
- Great for product demos

---

## üîß Adding Your Own Custom Model

### Step 1: Add to MODEL_CONFIGS

Edit `python_backend/server_multimodel.py`:

```python
MODEL_CONFIGS = {
    # ... existing models ...
    
    'your-model': {
        'model_id': 'username/your-model-name',
        'min_vram': 8,
        'type': 'diffusers',
        'description': 'Your custom model description'
    }
}
```

### Step 2: Add Loading Logic (if needed)

```python
def load_model(self):
    # ... existing code ...
    
    if self.model_name == 'your-model':
        # Custom loading if needed
        from your_package import YourPipeline
        self.pipeline = YourPipeline.from_pretrained(...)
```

### Step 3: Use It

```bash
VIDEO_MODEL=your-model
```

---

## üé® Model-Specific Features

### ModelScope (Best for 4GB GPU)

```python
# Optimized settings
{
    "prompt": "A beautiful sunset",
    "num_frames": 16,      # Lower for 4GB
    "width": 256,          # Lower resolution
    "height": 256,
    "num_inference_steps": 25  # Faster
}
```

### LTX-Video (Best Quality)

```python
# High quality settings
{
    "prompt": "A cinematic shot of mountains",
    "num_frames": 128,     # More frames
    "width": 768,          # Higher resolution
    "height": 768,
    "num_inference_steps": 50
}
```

### Stable Video Diffusion (Image‚ÜíVideo)

```python
# Image animation
POST /api/generate/image-to-video
- Upload image
- Get animated video
- No text prompt needed
```

---

## üì° New API Endpoints (Multi-Model Version)

### List Available Models

```bash
GET http://localhost:5000/api/models
```

Response:
```json
{
  "models": [
    {
      "name": "modelscope",
      "model_id": "damo-vilab/text-to-video-ms-1.7b",
      "min_vram_gb": 4,
      "description": "Works on 4GB GPU",
      "loaded": true
    },
    {
      "name": "ltx-video",
      "model_id": "Lightricks/LTX-Video",
      "min_vram_gb": 12,
      "description": "High quality, needs 12GB+",
      "loaded": false
    }
  ],
  "current_model": "modelscope"
}
```

### Switch Model

```bash
POST http://localhost:5000/api/switch-model
Content-Type: application/json

{
  "model_name": "ltx-video"
}
```

Response:
```json
{
  "message": "Switched to ltx-video",
  "model": {
    "model_id": "Lightricks/LTX-Video",
    "min_vram": 12
  }
}
```

---

## üéØ Recommended Configurations

### For Your 4GB GPU:

```bash
# .env configuration
VIDEO_MODEL=modelscope
ENABLE_GPU=true
LOW_MEMORY_MODE=true
MAX_FRAMES=16
DEFAULT_WIDTH=256
DEFAULT_HEIGHT=256
MAX_CONCURRENT_REQUESTS=1
```

### For Development/Testing:

```bash
# Fast iterations
VIDEO_MODEL=zeroscope
MAX_FRAMES=8
DEFAULT_WIDTH=128
DEFAULT_HEIGHT=128
num_inference_steps=10
```

### For Production (16GB+ GPU):

```bash
# Best quality
VIDEO_MODEL=ltx-video
MAX_FRAMES=128
DEFAULT_WIDTH=768
DEFAULT_HEIGHT=768
num_inference_steps=50
```

---

## üîÑ Migration Guide

### From Single-Model to Multi-Model

```bash
# 1. Backup current server
cp python_backend/server.py python_backend/server_original.py

# 2. Use multi-model version
cp python_backend/server_multimodel.py python_backend/server.py

# 3. Update config
cp .env.multimodel .env
nano .env  # Choose your model

# 4. Restart
# (restart both servers)
```

### Stay with Single-Model

If you only need one model:
```bash
# Keep using original server.py
# It's simpler and slightly faster for single model
```

---

## üéì Examples

### Example 1: Use ModelScope on 4GB GPU

```bash
# .env
VIDEO_MODEL=modelscope
ENABLE_GPU=true

# Generate
curl -X POST http://localhost:8080/api/v1/generate/text-to-video \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "A cute puppy playing in grass",
    "num_frames": 24,
    "width": 256,
    "height": 256
  }'
```

### Example 2: Switch Between Models

```bash
# Start with fast model for testing
VIDEO_MODEL=zeroscope

# Later switch to quality model
curl -X POST http://localhost:5000/api/switch-model \
  -d '{"model_name": "ltx-video"}'

# Generate with quality model
curl -X POST http://localhost:8080/api/v1/generate/text-to-video \
  -d '{"prompt": "Epic landscape", "width": 768, "height": 768}'
```

### Example 3: Image Animation

```bash
# Use Stable Video Diffusion
VIDEO_MODEL=svd

# Animate an image
curl -X POST http://localhost:8080/api/v1/generate/image-to-video \
  -F "image=@my_photo.jpg"
```

---

## ‚úÖ Summary

### Can use ANY video model?

**YES!** The project supports:

1. ‚úÖ **Built-in models** (5 models ready to use)
2. ‚úÖ **Any HuggingFace model** (add to config)
3. ‚úÖ **Custom models** (implement loader)
4. ‚úÖ **Dynamic switching** (change without restart)

### Which version to use?

| Use Case | Version | Files |
|----------|---------|-------|
| Only need LTX-Video | Single-model | `server.py` (current) |
| Want model flexibility | Multi-model | `server_multimodel.py` |
| 4GB GPU | Multi-model | Use `modelscope` |
| Multiple models | Multi-model | Switch via API |

### Files Created:

- ‚úÖ `python_backend/server_multimodel.py` - Multi-model support
- ‚úÖ `.env.multimodel` - Multi-model configuration
- ‚úÖ `examples/test_multimodel.sh` - Test script
- ‚úÖ `docs/MULTI_MODEL_SUPPORT.md` - Documentation
- ‚úÖ This guide!

**You're ready to use ANY video model!** üöÄ
