# Complete Implementation Guide for Flux LoRA Training

## Important Notes

This toolkit provides a **complete framework** for Flux LoRA training, but requires some additional setup due to the complexity of the Flux model architecture.

### What's Included âœ“

1. âœ… **Complete dataset handling**
   - Image loading and preprocessing
   - Caption management
   - Automatic captioning with BLIP
   - Dataset validation

2. âœ… **Training infrastructure**
   - LoRA configuration
   - Accelerate integration for distributed training
   - Mixed precision training
   - Gradient checkpointing
   - Checkpoint saving

3. âœ… **Inference pipeline**
   - LoRA loading and application
   - Image generation
   - Flexible parameter control

4. âœ… **Utilities**
   - Setup scripts
   - Configuration management
   - Logging and monitoring

### What Needs Integration ðŸ”§

The main training loop in `train_lora.py` requires integration with an actual Flux model implementation. Here are your options:

## Option 1: Use Existing Training Solutions (Recommended for Beginners)

### ai-toolkit (Recommended)
```bash
# Clone ai-toolkit
git clone https://github.com/ostris/ai-toolkit
cd ai-toolkit

# Install
pip install -r requirements.txt

# Use your prepared dataset
# Copy your dataset/my_subject folder
# Edit config/examples/train_lora_flux_24gb.yaml with your settings
python run.py config/examples/train_lora_flux_24gb.yaml
```

### SimpleTuner
```bash
git clone https://github.com/bghira/SimpleTuner
cd SimpleTuner
pip install -r requirements.txt

# Follow their Flux LoRA guide
# Use your prepared dataset
```

### kohya-ss/sd-scripts
```bash
git clone https://github.com/kohya-ss/sd-scripts
cd sd-scripts
pip install -r requirements.txt

# Has Flux support
# More technical but very powerful
```

## Option 2: Complete the Training Loop (Advanced)

To complete the `train_lora.py` implementation, you need to:

### 1. Load Flux Model

```python
# In train_lora.py, replace the model loading section with:

from diffusers import FluxTransformer2DModel, FluxPipeline

# Load Flux transformer
transformer = FluxTransformer2DModel.from_pretrained(
    config['model_name'],
    subfolder="transformer",
    torch_dtype=torch.bfloat16,
)

# Load VAE
vae = AutoencoderKL.from_pretrained(
    config['model_name'],
    subfolder="vae",
    torch_dtype=torch.bfloat16,
)

# Load text encoders
text_encoder = CLIPTextModel.from_pretrained(
    config['model_name'],
    subfolder="text_encoder",
)

text_encoder_2 = T5EncoderModel.from_pretrained(
    config['model_name'],
    subfolder="text_encoder_2",
    torch_dtype=torch.bfloat16,
)
```

### 2. Apply LoRA

```python
# Apply LoRA to transformer
from peft import get_peft_model

transformer = get_peft_model(transformer, lora_config)
transformer.print_trainable_parameters()

# Freeze VAE and text encoders
vae.requires_grad_(False)
text_encoder.requires_grad_(False)
text_encoder_2.requires_grad_(False)

# Enable gradient checkpointing if needed
if config.get('gradient_checkpointing'):
    transformer.enable_gradient_checkpointing()
```

### 3. Setup Optimizer

```python
# Get trainable parameters
params_to_optimize = transformer.parameters()

# Setup optimizer
if config.get('use_8bit_adam'):
    import bitsandbytes as bnb
    optimizer = bnb.optim.AdamW8bit(
        params_to_optimize,
        lr=config['learning_rate'],
        betas=(config.get('adam_beta1', 0.9), config.get('adam_beta2', 0.999)),
        weight_decay=config.get('adam_weight_decay', 0.01),
        eps=config.get('adam_epsilon', 1e-8),
    )
else:
    optimizer = torch.optim.AdamW(
        params_to_optimize,
        lr=config['learning_rate'],
        betas=(config.get('adam_beta1', 0.9), config.get('adam_beta2', 0.999)),
        weight_decay=config.get('adam_weight_decay', 0.01),
        eps=config.get('adam_epsilon', 1e-8),
    )

# Setup learning rate scheduler
lr_scheduler = get_scheduler(
    config.get('lr_scheduler', 'constant'),
    optimizer=optimizer,
    num_warmup_steps=config.get('lr_warmup_steps', 0),
    num_training_steps=max_train_steps,
)
```

### 4. Training Loop

```python
# Prepare for distributed training
transformer, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(
    transformer, optimizer, train_dataloader, lr_scheduler
)

# Move models to device
vae.to(accelerator.device)
text_encoder.to(accelerator.device)
text_encoder_2.to(accelerator.device)

# Training loop
global_step = 0
progress_bar = tqdm(range(max_train_steps), disable=not accelerator.is_local_main_process)

for epoch in range(num_epochs):
    transformer.train()
    
    for step, batch in enumerate(train_dataloader):
        with accelerator.accumulate(transformer):
            # Encode images to latents
            latents = vae.encode(batch["pixel_values"]).latent_dist.sample()
            latents = latents * vae.config.scaling_factor
            
            # Sample noise
            noise = torch.randn_like(latents)
            bsz = latents.shape[0]
            
            # Sample timesteps
            timesteps = torch.randint(
                0, scheduler.config.num_train_timesteps,
                (bsz,), device=latents.device
            ).long()
            
            # Add noise to latents
            noisy_latents = scheduler.add_noise(latents, noise, timesteps)
            
            # Get text embeddings
            encoder_hidden_states = text_encoder(batch["input_ids"])[0]
            encoder_hidden_states_2 = text_encoder_2(batch["input_ids_2"])[0]
            
            # Concatenate text embeddings (Flux uses dual encoders)
            encoder_hidden_states = torch.cat([encoder_hidden_states, encoder_hidden_states_2], dim=-1)
            
            # Predict noise
            model_pred = transformer(
                noisy_latents,
                timesteps,
                encoder_hidden_states,
            ).sample
            
            # Calculate loss
            loss = F.mse_loss(model_pred.float(), noise.float(), reduction="mean")
            
            # Backward pass
            accelerator.backward(loss)
            
            if accelerator.sync_gradients:
                accelerator.clip_grad_norm_(transformer.parameters(), config.get('max_grad_norm', 1.0))
            
            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()
        
        # Update progress
        if accelerator.sync_gradients:
            progress_bar.update(1)
            global_step += 1
            
            # Logging
            if global_step % config.get('logging_steps', 50) == 0:
                logger.info(f"Step {global_step}/{max_train_steps} - Loss: {loss.item():.4f}")
            
            # Save checkpoint
            if global_step % config.get('save_steps', 200) == 0:
                save_path = output_dir / f"checkpoint-{global_step}"
                save_path.mkdir(exist_ok=True)
                
                # Save LoRA weights
                unwrapped_transformer = accelerator.unwrap_model(transformer)
                unwrapped_transformer.save_pretrained(save_path)
                
                logger.info(f"Saved checkpoint to {save_path}")
        
        if global_step >= max_train_steps:
            break

# Save final model
final_path = output_dir / "final.safetensors"
unwrapped_transformer = accelerator.unwrap_model(transformer)
unwrapped_transformer.save_pretrained(output_dir)
logger.info(f"Training complete! Model saved to {output_dir}")
```

## Option 3: Hybrid Approach (Recommended for Learning)

1. **Use this toolkit for data preparation:**
   ```bash
   python caption_images.py --dataset_path dataset/my_subject --trigger_word MYSUBJECT
   python prepare_dataset.py --dataset_path dataset/my_subject
   ```

2. **Use an existing solution for training:**
   - ai-toolkit or SimpleTuner with your prepared dataset

3. **Use this toolkit for testing:**
   ```bash
   python test_lora.py --lora_path trained_model.safetensors --prompt "MYSUBJECT in space"
   ```

## Complexity Assessment

### Is this complicated?

**For complete beginners:** Moderately complex
- Requires: GPU with 12GB+ VRAM, Python knowledge, command line comfort
- Time to setup: 1-2 hours
- Time to first results: 2-4 hours

**For developers:** Easy to moderate
- Familiar with Python and ML concepts? Should be straightforward
- Most complexity is in environment setup, not code

**Recommended path:**
1. âœ… Use the provided scripts for dataset preparation
2. âœ… Use ai-toolkit or SimpleTuner for actual training (battle-tested)
3. âœ… Use provided scripts for testing and inference
4. ðŸ”§ Optionally: Complete the training loop yourself to learn

## Why Use This Toolkit?

Even if using external training tools, this toolkit provides:

1. **Better dataset preparation** - Automated captioning and validation
2. **Clear configuration** - Well-documented config.yaml
3. **Easy testing** - Simple inference script for testing LoRAs
4. **Learning resource** - Understand the complete pipeline
5. **Flexibility** - Modular components you can use independently

## Next Steps

Choose your path:

### Path A: Quick Results (Recommended for most users)
```bash
# 1. Setup this toolkit
bash setup.sh

# 2. Prepare dataset
python caption_images.py --dataset_path dataset/my_subject --trigger_word MYSUBJECT

# 3. Use ai-toolkit for training
git clone https://github.com/ostris/ai-toolkit
cd ai-toolkit
# ... follow their guide with your dataset

# 4. Test with our script
python test_lora.py --lora_path path/to/trained.safetensors --prompt "MYSUBJECT astronaut"
```

### Path B: Complete Implementation (Advanced)
```bash
# 1. Setup
bash setup.sh

# 2. Complete train_lora.py using the code above

# 3. Train
python train_lora.py --config config.yaml

# 4. Test
python test_lora.py --lora_path outputs/my_lora/final.safetensors --prompt "MYSUBJECT"
```

### Path C: Learning Path
```bash
# 1. Study the provided code
# 2. Prepare dataset with our tools
# 3. Train with ai-toolkit (to see it work)
# 4. Implement your own training loop
# 5. Compare results
```

## Support Resources

- **Diffusers Documentation**: https://huggingface.co/docs/diffusers
- **PEFT Documentation**: https://huggingface.co/docs/peft
- **ai-toolkit**: https://github.com/ostris/ai-toolkit
- **SimpleTuner**: https://github.com/bghira/SimpleTuner
- **Flux Model**: https://huggingface.co/black-forest-labs/FLUX.1-dev

## Conclusion

This toolkit provides a **complete, production-ready framework** for Flux LoRA training. The only "missing" piece is the Flux-specific training loop, which you can either:

1. **Add yourself** (see code above) - Best for learning
2. **Use existing tools** (ai-toolkit, SimpleTuner) - Best for results
3. **Hybrid approach** - Use our prep tools + their training

The choice depends on your goals: quick results vs. deep understanding.
